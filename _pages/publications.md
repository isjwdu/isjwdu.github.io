---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% if site.author.googlescholar %}
  <div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">Google Scholar</a>.</div>
  <div class="wordwrap">Name<sup>*</sup> represents equal contribution.</div>
{% endif %}

- **CodecFake-Omni: A Large-Scale Codec-based Deepfake Speech Dataset**<br/>
    <u>Jiawei Du<sup>*</sup></u>, Xuanjun Chen<sup>*</sup>, Haibin Wu, Lin Zhang, I-Ming Lin, I-Hsiang Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, Jyh-Shing Roger Jang, Hung-yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](https://arxiv.org/abs/2501.08238) ]

- **Empower Typed Descriptions by Large Language Models for Speech Emotion**<br/>
    Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, <u>Jiawei Du</u>, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-yi Lee<br/>
    *APSIPA ASC 2024*<br/>
    [ [pdf](https://www.researchgate.net/publication/384833473_Empower_Typed_Descriptions_by_Large_Language_Models_for_Speech_Emotion_Recognition) ]

- **DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset**<br/>
    <u>Jiawei Du<sup>*</sup></u>, I-Ming Lin<sup>\*</sup>, I-Hsiang Chiu<sup>\*</sup>, Xuanjun Chen, Haibin Wu, Wenze Ren, Yu Tsao, Hung-yi Lee, Jyh-Shing Roger Jang<br/>
    *SLT 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2409.08731) | [demo](https://huggingface.co/datasets/isjwdu/DFADD) | [code](https://github.com/isjwdu/DFADD) ]

- **Open-Emotion: A Reproducible EMO-SUPERB for Speech Emotion Recognition Systems**<br/>
    Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, <u>Jiawei Du</u>, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-yi Lee<br/>
    *SLT 2024*<br/>
    [ [pdf](https://www.researchgate.net/publication/383736295_OPEN-EMOTION_A_REPRODUCIBLE_EMO-SUPERB_FOR_SPEECH_EMOTION_RECOGNITION_SYSTEMS) ]

- **Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural codec models**<br/>
    Haibin Wu, <u>Jiawei Du<sup>*</sup></u>, Xuanjun Chen<sup>\*</sup>, Yi-Cheng Lin<sup>\*</sup>, Kai-Wei Chang<sup>\*</sup>, Ke-Han Lu<sup>\*</sup>, Alexander Liu<sup>\*</sup>, Ho Lam Chung<sup>\*</sup>, Yuan-Kuei Wu<sup>\*</sup>, Dongchao Yang<sup>\*</sup>, Songxiang Liu, Yi-Chiao Wu, Xu Tan, James Glass, Shinji Watanabe, Hung-yi Lee<br/>
    *SLT 2024*<br/>
    [ [pdf](https://arxiv.org/abs/2409.14085) | [code](https://codecsuperb.github.io/) ]

- **Neural Codec-based Adversarial Sample Detection for Speaker Verification**<br/>
    <u>Jiawei Du<sup>*</sup></u>, Xuanjun Chen<sup>\*</sup>,Haibin Wu, Jyh-Shing Roger Jang, Hung-yi Lee<br/>
    *Interspeech 2024*<br/>
    [ [pdf](https://www.isca-archive.org/interspeech_2024/chen24p_interspeech.html) ]

- **EMO-SUPERB: An In-depth Look at Speech Emotion Recognition**<br/>
    Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, <u>Jiawei Du</u>, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee<br/>
    *Preprint*<br/>
    [ [pdf](http://arxiv.org/abs/2402.13018) | [webpage](http://emosuperb.github.io/) | [code](http://github.com/EMOsuperb/EMO-SUPERB-submission) ]

- **Dcase 2023 task 6b: Text-to-audio retrieval using pretrained models**<br/>
    <u>Jiawei Du<sup>*</sup></u>, Chung-Che Wang<sup>\*</sup>, Jyh-Shing Roger Jang<br/>
    *DCASE2023 Challenge, Tech. Rep*<br/>
    [ [pdf](http://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Wang_40_t6b.pdf) ]
